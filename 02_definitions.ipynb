{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports, definitions, and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import re\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the spaCy library and load the language model\n",
    "The [`spaCy`](https://spacy.io/) library provides pre-built natural language processing tools and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "\n",
    "# Load the model\n",
    "NLP = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the dataset path constants\n",
    "The following code will store the relative paths of each provided dataset in a constant for reuse throughout this notebook. These files are in the `data` folder that was downloaded alongside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Susan B. Anthony\n",
    "ANTHONY = \"data/anthony/susan-b-anthony-papers_2022-10-12.csv\"\n",
    "SPEECHES = \"data/anthony/anthony_speech_list.csv\"\n",
    "\n",
    "# Carrie Chapman Catt\n",
    "CATT = \"data/catt/carrie-chapman-catt-papers_2022-10-12.csv\"\n",
    "\n",
    "# Elizabeth Cady Stanton\n",
    "STANTON = \"data/stanton/elizabeth-cady-stanton-papers_2022-10-19.csv\"\n",
    "\n",
    "# Mary Church Terrell\n",
    "TERRELL = \"data/terrell/mary-church-terrell-advocate-for-african-americans-and-women_2023-01-20.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define processing functions\n",
    "The following helper functions were written to process the data. They are loaded here to make the later sections easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load each CSV file into a data frame.\n",
    "    \n",
    "    Returns:\n",
    "        df (data frame): A data frame containing the data loaded from csv.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file, dtype=str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def string_to_filename(string: str) -> str:\n",
    "    \"\"\"Converts an arbitrary string into a valid filename\"\"\"\n",
    "\n",
    "    s = re.sub('[^0-9a-z]+', '_', string.lower())\n",
    "    filename = re.sub(r\"\\_+\", '_', s)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def read_cache(id: str) -> pd.DataFrame:\n",
    "    \"\"\"Read a data frame that was cached to file\n",
    "\n",
    "    Returns:\n",
    "        df (data frame): A data frame containing previously cached data\n",
    "    \"\"\"\n",
    "    filename = string_to_filename(id)\n",
    "    df = pd.read_pickle(f\"outputs/{filename}.pkl\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_cache(df: pd.DataFrame, id: str) -> None:\n",
    "    \"\"\"Cache a data frame to file\"\"\"\n",
    "\n",
    "    filename = string_to_filename(id)\n",
    "    df.to_pickle(f\"outputs/{filename}.pkl\")\n",
    "\n",
    "\n",
    "def tokens(text) -> list:\n",
    "    \"\"\"Runs NLP process on text input. \n",
    "    \n",
    "    Returns: \n",
    "        process (list): A list containing tuples of NLP attributes \n",
    "            for each word in the transcription.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = NLP(str(text))\n",
    "    process = ([(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                token.shape_, token.is_alpha, token.is_stop) for token in doc])\n",
    "\n",
    "    return process\n",
    "\n",
    "\n",
    "def entities(text) -> list:\n",
    "    \"\"\"Runs NER process on text input. \n",
    "    \n",
    "    Returns:\n",
    "        process (list): A list containing tuples of NER attributes \n",
    "            for each word in the transciption.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = NLP(str(text))\n",
    "    process = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return process\n",
    "\n",
    "\n",
    "def separate_text(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Adds new columns to the data frame then loops through the \n",
    "    tokenized text of each row moving each category to the newly \n",
    "    created relevant column.\"\"\"\n",
    "    \n",
    "    # Add new columns (c) to the data frame\n",
    "    for c in ['text', \n",
    "              'stop_words', \n",
    "              'nonalphanums', \n",
    "              'numbers', \n",
    "              'ambigs', \n",
    "              'processed_text']:\n",
    "        df[c] = pd.Series(dtype=str)\n",
    "    \n",
    "    # Iterates over a copy of tokenized_text to filter words\n",
    "    # into five categories\n",
    "    for row in range(df.shape[0]):\n",
    "        text_block = df['tokenized_text'].iloc[row].copy()\n",
    "        \n",
    "        text = []\n",
    "        stop_words = []\n",
    "        nonalphanums = []\n",
    "        numbers = []\n",
    "        ambigs = []\n",
    "    \n",
    "        for idx, word in enumerate(text_block):\n",
    "            # Move stopwords\n",
    "            if word[7] == True:\n",
    "                stop_words.append(text_block[idx])\n",
    "            # Move punctuation and whitespace\n",
    "            elif word[2] in ['PUNCT', 'SPACE', 'CCONJ', 'X', 'SYM']:\n",
    "                nonalphanums.append(text_block[idx])\n",
    "            # Move numbers\n",
    "            elif word[2] == 'NUM':\n",
    "                numbers.append(text_block[idx])\n",
    "            # Move ambiguous transcribed words\n",
    "            elif '?' in word[5]:\n",
    "                ambigs.append(text_block[idx])\n",
    "            # Move text\n",
    "            else:\n",
    "                text.append(text_block[idx])\n",
    "                \n",
    "        df['text'].iloc[row] = text\n",
    "        df['stop_words'].iloc[row] = stop_words\n",
    "        df['nonalphanums'].iloc[row] = nonalphanums\n",
    "        df['numbers'].iloc[row] = numbers\n",
    "        df['ambigs'].iloc[row] = ambigs\n",
    "        # Create a processed_text column containing lowercase lemmas \n",
    "        # for all words in list 'text'\n",
    "        df['processed_text'].iloc[row] = [i[1].lower() for i in df['text'].iloc[row]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
