{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<details>\n",
    "<summary>About the data</summary>\n",
    "\n",
    "## About the data\n",
    "\n",
    "This tutorial provides an example of data processing and visualization using transcription data from By the People, the Library of Congress's crowdsourced transcriptioning program. By the People invites anyone to contribute to the Library of Congress as a virtual volunteer by transcribing and reviewing digital images of texts to enhance Library of Congress digital collections. The Library of Congress thanks all By the People volunteers for sharing their time and knowledge with us to make this tutorial possible. \n",
    "\n",
    "By the People transcriptions and tags are created by anonymous and registered volunteers. Once a transcription is finished, it must be reviewed by a registered volunteer. A transcription may undergo multiple rounds of edits before being completed. Finally, transcriptions are spot-checked by Library of Congress subject matter experts before they are incorporated into the digital collections on the Library's website to enhance search and accessibility. Transcriptions are also packaged into .csv files and made available as datasets as part of the [Selected Datasets Collection](https://www.loc.gov/collections/selected-datasets/).\n",
    "\n",
    "In this tutorial we will work with four datasets related to the movement for women's suffrage in the United States. Each dataset’s README includes additional information about its content and creation\n",
    "- Anthony, Susan B. Transcription datasets from Susan B. Anthony Papers, Manuscript Division. compiled by By The People. Washington, D.C.: By the People, Library of Congress, to 2022, 2021. Software, E-Resource. https://www.loc.gov/item/2020445591/.\n",
    "- Catt, Carrie Chapman. Transcription datasets from Carrie Chapman Catt Papers, Manuscript Division. compiled by By The People. Washington, D.C.: By the People, Library of Congress, to 2022, 2020. Software, E-Resource. https://www.loc.gov/item/2019667239/.\n",
    "- Stanton, Elizabeth Cady. Transcription datasets from Elizabeth Cady Stanton Papers, Manuscript Division. compiled by By The People. Washington, D.C.: By the People, Library of Congress, 2021. Software, E-Resource. https://www.loc.gov/item/2020445592/.\n",
    "- Terrell, Mary Church. Transcription dataset from the Mary Church Terrell Papers, Manuscript Division. compiled by By The People. Washington, D.C.: By the People, Library of Congress, to 2021, 2018. Software, E-Resource. https://www.loc.gov/item/2021387726/.\n",
    "\n",
    "Transcription volunteers are instructed to transcribe the text as written, including misspellings and abbreviations. Formatting is generally not preserved with the exception of line breaks. Minimal markup does include “?” for illegible or unclear text, square brackets around deleted text, and square brackets and asterisks around marginalia `([*example*])`. Pages without text are marked “nothing to transcribe” and do not have transcriptions in [loc.gov](https://www.loc.gov/).\n",
    "\n",
    "By the People datasets contain the following fields:\n",
    "- Campaign – this is the highest hierarchical level in the arrangement of collections on By the People (example: [Susan B. Anthony Papers](https://crowd.loc.gov/campaigns/susan-b-anthony-papers/)). This field displays the campaign’s title.\n",
    "- Project – this is the second-highest hierarchical level of collections on By the People. Projects may map to an existing subset of a digital collection, such as an archival series, or may be a grouping of related items uniquely organized for By the People. This field displays the project’s title.\n",
    "- Item – this is the third-highest hierarchical level of collections on By the People, typically representing a folder, letter, document, or diary. This field displays the item title. \n",
    "- ItemId – this is the identifier for the item (see above for definition). This numerical identifier is consistent across the By the People website and in loc.gov. The item and metadata are usually located on the Library’s website at `https://www.loc.gov/item/[ItemID]/`\n",
    "- Asset – this is the identifier for the individual asset image. It is also referred to colloquially as the “page” by By the People volunteers and Community Managers. This identifier is used in the By the People site and on loc.gov. \n",
    "- AssetStatus – this indicates the status of the asset in the peer review workflow – Not Started, In Progress, Needs Review, or Completed. Dataset assets will always be marked as “Completed.” \n",
    "- DownloadURL – this link provides access to the image file for the Asset from which the transcription was created.\n",
    "- Transcription – this is the text created by the By the People volunteers, representing the written content of the DownloadURL image and corresponding to the Asset. This field will be blank for assets that volunteers marked “Nothing to transcribe”.\n",
    "- Tags – these are all the tags that have been applied to the asset. If there is more than one tag, the tags are delimited by a semicolon and space.\n",
    "\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<details>\n",
    "<summary>About the notebook</summary>\n",
    "\n",
    "## About the notebook\n",
    "\n",
    "This tutorial first cleans and processes transcriptions from the four datasets using [Pandas](https://pandas.pydata.org/) and the [spaCy](https://spacy.io/) Natural Language Processing library. This code tokenizes the transcriptions, breaking the strings of text into tokens (words) that will be further analyzed. It then identifies the lemma, or root, for each word. For example, the lemma of \"voted\" is \"vote\", and the lemma of \"women\" is \"woman\". The code next iterates over each token to produce a list of lemmas from the original transcriptions that excludes stop words, punctuation, numbers, and words that volunteers were unable to fully transcribe, which are designated with \"?\". Stop words are commonly used words, such as \"the\", \"a\", or \"is\".\n",
    "\n",
    "The tutorial then creates two visualizations from the cleaned data using the [Matplotlib](https://matplotlib.org/) and [Numpy](https://numpy.org/) Python libraries. The first is a combined bar graph showing the five most used words for each of the four datasets. The second is a focused look at the \"Speeches\" series from the Susan B. Anthony Papers. With data coming from a [typed inventory of speeches](http://hdl.loc.gov/loc.mss/ms997009.mss11049.036) found in the collection, this code groups Anthony's speeches by year, and then plots the usage of the top five words in her speeches by year.\n",
    "\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<details>\n",
    "<summary>Running the notebook</summary>\n",
    "    \n",
    "## Running the notebook\n",
    "\n",
    "In order to run a Jupyter notebook, navigate to the directory that contains the notebook files using `cd /path/to/dcm-btp-notebooks`, then run the command `jupyter notebook`. This will launch the Notebook Dashboard in an Internet browser.\n",
    "\n",
    "In order to properly run these notebooks, make sure that the appropriate Python libraries are installed. Further information can be found in the README file. The dataset files are already included in this tutorial in the `data` directory (along with each dataset’s README), which can be seen in the Notebook Dashboard.\n",
    "\n",
    "The entire notebook can be run by clicking `Run` in the menu bar. Individual cells can be run by clicking into the cell, then hitting `Shift + Enter`.\n",
    "\n",
    "The notebook contains optional code that can be run to print results to the notebook. This helps show what the code is doing at each step. These cells have \"Optional:\" in the title. Remove `#` from the code to un-comment and run those lines of code.\n",
    "\n",
    "The outputs from the tutorial will be saved to the `outputs` directory, which can be seen in the Notebook Dashboard.\n",
    "\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Authorship and use</summary>\n",
    "    \n",
    "## Authorship and use\n",
    "\n",
    "These notebooks were created by Dave Durden and Madeline Goebel, Digital Collection Specialists at the Library of Congress. They are made available under the [Creative Commons Attribution license](https://creativecommons.org/licenses/by/4.0/legalcode).\n",
    "\n",
    "All contributions to the By the People application are released into the public domain as they are created. Anyone can use and re-use the datasets in any way they want.\n",
    "\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "This section contains the code necessary to load and transform transcription data; it is divided into four parts:\n",
    "1. Import statements and function definitions,\n",
    "2. Text tokenization for all datasets,\n",
    "3. Data processing for the Susan B. Anthony Speeches subset,\n",
    "4. Data processing for the Susan B. Anthony, Carrie Chapman Catt, Elizabeth Cady Stanton, and Mary Church Terrell transciption data.\n",
    "\n",
    "## Instructions for running the code\n",
    "1. Run the cells in order.\n",
    "2. Optional code is available for previewing the data during each step of processing; these lines are not necessary for processing the data, but may be useful for those who wish to see how the data changes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import modules, set constants, define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import re\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import the spaCy library and load the language model\n",
    "The [`spaCy`](https://spacy.io/) library provides pre-built natural language processing tools and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "\n",
    "# Load the model\n",
    "NLP = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set the dataset path constants\n",
    "The following code will store the relative paths of each provided dataset in a constant for reuse throughout this notebook. These files are in the `data` folder that was downloaded alongside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Susan B. Anthony\n",
    "ANTHONY = \"data/anthony/susan-b-anthony-papers_2022-10-12.csv\"\n",
    "SPEECHES = \"data/anthony/anthony_speech_list.csv\"\n",
    "\n",
    "# Carrie Chapman Catt\n",
    "CATT = \"data/catt/carrie-chapman-catt-papers_2022-10-12.csv\"\n",
    "\n",
    "# Elizabeth Cady Stanton\n",
    "STANTON = \"data/stanton/elizabeth-cady-stanton-papers_2022-10-19.csv\"\n",
    "\n",
    "# Mary Church Terrell\n",
    "TERRELL = \"data/terrell/mary-church-terrell-advocate-for-african-americans-and-women_2023-01-20.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define processing functions\n",
    "The following helper functions were written to process the data. They are loaded here to make the later sections easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load each CSV file into a data frame.\n",
    "    \n",
    "    Returns:\n",
    "        df (data frame): A data frame containing the data loaded from csv.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file, dtype=str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def tokens(text) -> list:\n",
    "    \"\"\"Runs NLP process on text input. \n",
    "    \n",
    "    Returns: \n",
    "        process (list): A list containing tuples of NLP attributes \n",
    "            for each word in the transcription.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = NLP(str(text))\n",
    "    process = ([(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                token.shape_, token.is_alpha, token.is_stop) for token in doc])\n",
    "\n",
    "    return process\n",
    "\n",
    "\n",
    "def entities(text) -> list:\n",
    "    \"\"\"Runs NER process on text input. \n",
    "    \n",
    "    Returns:\n",
    "        process (list): A list containing tuples of NER attributes \n",
    "            for each word in the transciption.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = NLP(str(text))\n",
    "    process = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return process\n",
    "\n",
    "\n",
    "def separate_text(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Adds new columns to the data frame then loops through the \n",
    "    tokenized text of each row moving each category to the newly \n",
    "    created relevant column.\"\"\"\n",
    "    \n",
    "    # Add new columns (c) to the data frame\n",
    "    for c in ['text', \n",
    "              'stop_words', \n",
    "              'nonalphanums', \n",
    "              'numbers', \n",
    "              'ambigs', \n",
    "              'processed_text']:\n",
    "        df[c] = pd.Series(dtype=str)\n",
    "    \n",
    "    # Iterates over a copy of tokenized_text to filter words\n",
    "    # into five categories\n",
    "    for row in range(df.shape[0]):\n",
    "        text_block = df['tokenized_text'].iloc[row].copy()\n",
    "        \n",
    "        text = []\n",
    "        stop_words = []\n",
    "        nonalphanums = []\n",
    "        numbers = []\n",
    "        ambigs = []\n",
    "    \n",
    "        for idx, word in enumerate(text_block):\n",
    "            # Move stopwords\n",
    "            if word[7] == True:\n",
    "                stop_words.append(text_block[idx])\n",
    "            # Move punctuation and whitespace\n",
    "            elif word[2] in ['PUNCT', 'SPACE', 'CCONJ', 'X', 'SYM']:\n",
    "                nonalphanums.append(text_block[idx])\n",
    "            # Move numbers\n",
    "            elif word[2] == 'NUM':\n",
    "                numbers.append(text_block[idx])\n",
    "            # Move ambiguous transcribed words\n",
    "            elif '?' in word[5]:\n",
    "                ambigs.append(text_block[idx])\n",
    "            # Move text\n",
    "            else:\n",
    "                text.append(text_block[idx])\n",
    "                \n",
    "        df['text'].iloc[row] = text\n",
    "        df['stop_words'].iloc[row] = stop_words\n",
    "        df['nonalphanums'].iloc[row] = nonalphanums\n",
    "        df['numbers'].iloc[row] = numbers\n",
    "        df['ambigs'].iloc[row] = ambigs\n",
    "        # Create a processed_text column containing lowercase lemmas \n",
    "        # for all words in list 'text'\n",
    "        df['processed_text'].iloc[row] = [i[1].lower() for i in df['text'].iloc[row]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Load each transcription dataset into a data frame\n",
    "The `load_csv` function will read the data from each path constant and store data in a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_csv(ANTHONY)\n",
    "c = load_csv(CATT)\n",
    "s = load_csv(STANTON)\n",
    "t = load_csv(TERRELL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Optional: Preview the first five lines of a loaded dataset\n",
    "Uncomment the line for the dataset you wish to preview and then run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anthony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Catt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stanton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Terrell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text tokenization for all four transcription datasets\n",
    "\n",
    "This section contains code that will tokenize the transcription data and add new columns to the data frames for each transcription dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a new column containing the output of the `tokens` function\n",
    "The `tokens` function uses the previously loaded spaCy model to analyze each word in the transcription. This results in several values for each word, including the lemma, the part-of-speech tag, the shape of the word, and whether it is a stop word or number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will take a while to run\n",
    "for dataset in [a, c, s, t]:\n",
    "    print(f\"Tokenizing text for dataset: {dataset['Campaign'][0]}\")\n",
    "    dataset['tokenized_text'] = dataset['Transcription'].apply(tokens)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create a new column containing the output of the `entities` function\n",
    "The `entities` function uses the previously loaded spaCy model to identify persons, places, organizations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will take a while to run\n",
    "for dataset in [a, c, s, t]:\n",
    "    print(f\"Identifying entities for dataset: {dataset['Campaign'][0]}\")\n",
    "    dataset['entities'] = dataset['Transcription'].apply(entities)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.2.1 Optional: Preview the results of the `tokens` and `entities` functions for the first row of a dataset\n",
    "Uncomment the lines in a cell for a dataset and then run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anthony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.head(1)\n",
    "#a['tokenized_text'].iloc[0]\n",
    "#a['entities'].iloc[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Catt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.head(1)\n",
    "#c['tokenized_text'].iloc[0]\n",
    "#c['entities'].iloc[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stanton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s.head(1)\n",
    "#s['tokenized_text'].iloc[0]\n",
    "#s['entities'].iloc[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Terrell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.head(1)\n",
    "#t['tokenized_text'].iloc[0]\n",
    "#t['entities'].iloc[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run the `separate_text` function to isolate tokens by category\n",
    "\n",
    "The `separate_text` function uses labels generated by the `spaCy` library to organize the contents of each transcription into actual text, stop words (conjunctions, prepositions, etc.), non-alphanumeric strings (punctuation, whitespace, etc.), numbers, and ambiguous words (when a transcriber cannot make out a word or character, a `?` will be used for the unknown character(s); this is reflected in the analyzed pattern of the word which is used to remove these words from the text category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the separate_text function on the Anthony data frame\n",
    "for dataset in [a, c, s, t]:\n",
    "    print(f\"Organizing tokens by category for: {dataset['Campaign'][0]}\")\n",
    "    separate_text(dataset)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.3.1 Optional: Preview the results for the first six rows of the updated data frame\n",
    "Uncomment the line for the dataset you wish to preview and then run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anthony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.iloc[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Catt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.iloc[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stanton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s.iloc[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Terrell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t.iloc[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process the Susan B. Anthony speech subset\n",
    "A [typed inventory of speeches](http://hdl.loc.gov/loc.mss/ms997009.mss11049.036) in the Susan B. Anthony Papers is available; this allows for subsetting the transcription data so that it can be processed and visualized separately from the entire transcription dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Extract the transcription text for the speeches\n",
    "The code below will group transcription data by the `ItemId`. The speech inventory will then be used to subset the transcription data using the `ItemId` of known speeches. The transcribed text will then be combined at the item level and stored in a dictionary that lists the `id`, `year`, `title`, and `text` of each speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the speech inventory\n",
    "a_speeches = load_csv(SPEECHES)\n",
    "\n",
    "# Group transcriptions by ItemId\n",
    "# Creates a dictionary where the ItemId is the key and the value is a list of associated row indexes\n",
    "a_groups = a.groupby('ItemId').groups\n",
    "\n",
    "# Create a list of dictionaries representing each speech\n",
    "# This structure is specifically designed for visualization in the next notebook\n",
    "speech_list = []\n",
    "\n",
    "for row in range(a_speeches.shape[0]):\n",
    "    d = re.findall('\\d{4}', a_speeches.iloc[row][1])\n",
    "    speech_id = a_speeches.iloc[row][0]\n",
    "    speech_text = []\n",
    "    for i in a_groups[speech_id]:\n",
    "        speech_text.extend(a['processed_text'].iloc[i])\n",
    "    speech = {'id': speech_id, \n",
    "              'year': d[0], \n",
    "              'title': a_speeches.iloc[row][2], \n",
    "              'text': speech_text}\n",
    "    speech_list.append(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Optional: Save `speech_list` to a Python file for import or reuse beyond these notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional code to create a stand-alone file for the speech data\n",
    "with open('outputs/anthony_speech_lemmas.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"#! usr/bin/env python3\\n#-*- encoding: utf-8 -*-\\n\\n\")\n",
    "    f.write(\"speech_list = \" + str(speech_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process transcription data for all four datasets\n",
    "The following code will prepare the data similar to the Susan B. Anthony speech subset above. Running this code is necessary for visualizing at the dataset-level for all four datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create a list of all words from `processed_text` for each dataset\n",
    "This code will create a dictionary containing the titles and aggregated text from the `processed_text` column for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = []\n",
    "\n",
    "for dataset in [a, c, s, t]:\n",
    "    transcription_text = []\n",
    "    for row in range(dataset.shape[0]):\n",
    "        transcription_text.extend(dataset['processed_text'].iloc[row])\n",
    "    transcription = {'title': dataset['Campaign'][0],\n",
    "                     'text': transcription_text}\n",
    "    transcriptions.append(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Optional: Save `transcriptions` to a Python file for import or reuse beyond these notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional code to create a stand-alone file for the processed transcriptions data\n",
    "with open('outputs/transcriptions_lemmas.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"#! usr/bin/env python3\\n#-*- encoding: utf-8 -*-\\n\\n\")\n",
    "    f.write(\"transcriptions = \" + str(transcriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data\n",
    "This section contains the code necessary to visualize the processed transcription data; it is divided into two parts:\n",
    "1. Create simple bar graphs,\n",
    "2. Create grouped bar graph.\n",
    "\n",
    "## Instructions for running the code\n",
    "1. Run the cells in order.\n",
    "2. If only creating the grouped bar graph, run cell `1.1 Import modules`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create simple bar graphs\n",
    "This code will create one bar graph for each dataset depicting five most frequent words from that dataset. The four graphs will be displayed in a 2x2 grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Plot data in bar graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "#This code will loop over the lemmas from each dataset.\n",
    "for i, transcription in enumerate(transcriptions):\n",
    "    title = transcription['title']\n",
    "    text = transcription['text']\n",
    "    word_counts = Counter(text)\n",
    "    #Identify the top 5 words for each dataset.\n",
    "    top_words = word_counts.most_common(5)\n",
    "    words, counts = zip(*top_words)\n",
    "    \n",
    "    #Create a 2x2 grid of bar graphs.\n",
    "    ax = axs[i // 2][i % 2]\n",
    "    ax.bar(words, counts)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Words')\n",
    "    ax.set_ylabel('Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create grouped bar graph\n",
    "This code will create a grouped bar graph showing the usage of the five most frequent words from Susan B. Anthony's speeches by year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Group speeches by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some years have multiple speeches\n",
    "year_speeches = {}\n",
    "for speech in speech_list:\n",
    "    year = speech[\"year\"]\n",
    "    if year not in year_speeches:\n",
    "        year_speeches[year] = []\n",
    "    year_speeches[year].append(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Count word occurences for each year, excluding \"nan\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan means \"not a number.\" This will exclude cases where there are no speeches for a given year.\n",
    "year_word_counts = {}\n",
    "for year, speeches in year_speeches.items():\n",
    "    word_counts = Counter()\n",
    "    for speech in speeches:\n",
    "        words = [word for word in speech[\"text\"] if word != \"nan\"]\n",
    "        word_counts.update(words)\n",
    "    year_word_counts[year] = word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Sum word occurences across all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter()\n",
    "for year_counts in year_word_counts.values():\n",
    "    word_counts += year_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Get and print five most frequent words with most occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = [word for word, count in word_counts.most_common(5)]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Create grouped bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, word in enumerate(top_words):\n",
    "    word_data = []\n",
    "    for year, word_counts in year_word_counts.items():\n",
    "        count = word_counts.get(word, 0)\n",
    "        word_data.append(count)\n",
    "    data.append(word_data)\n",
    "\n",
    "bar_width = 0.15\n",
    "year_labels = list(year_word_counts.keys())\n",
    "x = np.arange(len(year_labels))\n",
    "fig, ax = plt.subplots()\n",
    "colors = ['tab:green', 'tab:orange', 'tab:blue', 'tab:red', 'tab:purple']\n",
    "for i, word_data in enumerate(data):\n",
    "    ax.bar(x - (2 - i) * bar_width, word_data, bar_width, label=top_words[i], color=colors[i])\n",
    "\n",
    "\n",
    "# Set the x-axis tick locations and labels\n",
    "ax.set_xticks(range(len(year_labels)))\n",
    "ax.set_xticklabels([int(year) for year in year_labels])\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Speeches by year and top words')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
