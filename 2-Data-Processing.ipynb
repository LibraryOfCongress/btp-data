{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Notebook\n",
    "\n",
    "This notebook contains the code necessary to load and transform transcription data; it is divided into four parts: \n",
    "\n",
    "1. Import statements and function definitions,\n",
    "2. Text tokenization for all datasets,\n",
    "3. Data processing for the Susan B. Anthony Speeches subset,\n",
    "4. Data processing for Susan B. Anthony, Carrie Chapman Catt, Elizabet Cady Stanton, and Mary Church Terrell transcription data.\n",
    "\n",
    "## Instructions for running the code\n",
    "1. Run the cells in order.\n",
    "2. Optional code is avaiable for previewing the data during each step of processing; these lines are not necessary for processing the data, but may be useful for those who wish to see how the data changes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import modules, set constants, define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import re\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import the spaCy library and load the language model\n",
    "The [`spaCy`](https://spacy.io/) library provides pre-built natural language processing tools and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "\n",
    "# Load the model\n",
    "NLP = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set the dataset path constants\n",
    "The following code will store the relative paths of each provided dataset in a constant for reuse throughout this notebook. These files are in the `data` folder that was downloaded alongside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Susan B. Anthony\n",
    "ANTHONY = \"data/anthony/susan-b-anthony-papers_2022-10-12.csv\"\n",
    "SPEECHES = \"data/anthony/anthony_speech_list.csv\"\n",
    "\n",
    "# Carrie Chapman Catt\n",
    "CATT = \"data/catt/carrie-chapman-catt-papers_2022-10-12.csv\"\n",
    "\n",
    "# Elizabeth Cady Stanton\n",
    "STANTON = \"data/stanton/elizabeth-cady-stanton-papers_2022-10-19.csv\"\n",
    "\n",
    "# Mary Church Terrell\n",
    "TERRELL = \"data/terrell/mary-church-terrell-advocate-for-african-americans-and-women_2023-01-20.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define processing functions\n",
    "The following helper functions were written to process the data. They are loaded here to make the later sections easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load each CSV file into a data frame.\n",
    "    \n",
    "    Returns:\n",
    "        df (data frame): A data frame containing the data loaded from csv.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file, dtype=str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def tokens(text) -> list:\n",
    "    \"\"\"Runs NLP process on text input. \n",
    "    \n",
    "    Returns: \n",
    "        process (list): A list containing tuples of NLP attributes \n",
    "            for each word in the transcription.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = NLP(str(text))\n",
    "    process = ([(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                token.shape_, token.is_alpha, token.is_stop) for token in doc])\n",
    "\n",
    "    return process\n",
    "\n",
    "\n",
    "def entities(text) -> list:\n",
    "    \"\"\"Runs NER process on text input. \n",
    "    \n",
    "    Returns:\n",
    "        process (list): A list containing tuples of NER attributes \n",
    "            for each word in the transciption.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = NLP(str(text))\n",
    "    process = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return process\n",
    "\n",
    "\n",
    "def separate_text(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Adds new columns to the data frame then loops through the \n",
    "    tokenized text of each row moving each category to the newly \n",
    "    created relevant column.\"\"\"\n",
    "    \n",
    "    # Add new columns (c) to the data frame\n",
    "    for c in ['text', \n",
    "              'stop_words', \n",
    "              'nonalphanums', \n",
    "              'numbers', \n",
    "              'ambigs', \n",
    "              'processed_text']:\n",
    "        df[c] = pd.Series(dtype=str)\n",
    "    \n",
    "    # Iterates over a copy of tokenized_text to filter words\n",
    "    # into five categories\n",
    "    for row in range(df.shape[0]):\n",
    "        text_block = df['tokenized_text'].iloc[row].copy()\n",
    "        \n",
    "        text = []\n",
    "        stop_words = []\n",
    "        nonalphanums = []\n",
    "        numbers = []\n",
    "        ambigs = []\n",
    "    \n",
    "        for idx, word in enumerate(text_block):\n",
    "            # Move stopwords\n",
    "            if word[7] == True:\n",
    "                stop_words.append(text_block[idx])\n",
    "            # Move punctuation and whitespace\n",
    "            elif word[2] in ['PUNCT', 'SPACE', 'CCONJ', 'X', 'SYM']:\n",
    "                nonalphanums.append(text_block[idx])\n",
    "            # Move numbers\n",
    "            elif word[2] == 'NUM':\n",
    "                numbers.append(text_block[idx])\n",
    "            # Move ambiguous transcribed words\n",
    "            elif '?' in word[5]:\n",
    "                ambigs.append(text_block[idx])\n",
    "            # Move text\n",
    "            else:\n",
    "                text.append(text_block[idx])\n",
    "                \n",
    "        df['text'].iloc[row] = text\n",
    "        df['stop_words'].iloc[row] = stop_words\n",
    "        df['nonalphanums'].iloc[row] = nonalphanums\n",
    "        df['numbers'].iloc[row] = numbers\n",
    "        df['ambigs'].iloc[row] = ambigs\n",
    "        # Create a processed_text column containing lowercase lemmas \n",
    "        # for all words in list 'text'\n",
    "        df['processed_text'].iloc[row] = [i[1].lower() for i in df['text'].iloc[row]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Load each transcription dataset into a data frame\n",
    "The `load_csv` function will read the data from each path constant and store data in a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_csv(ANTHONY)\n",
    "c = load_csv(CATT)\n",
    "s = load_csv(STANTON)\n",
    "t = load_csv(TERRELL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Optional: Preview the first five lines of a loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the line for the dataset you wish to preview and then run the cell\n",
    "#a.head()\n",
    "#c.head()\n",
    "#s.head()\n",
    "#t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text tokenization for all four transcription datasets\n",
    "\n",
    "This section contains code that will tokenize the transcription data and add new columns to the data frames for each transcription dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a new column containing the output of the `tokens` function\n",
    "The `tokens` function uses the previously loaded spaCy model to analyze each word in the transcription. This results in several values for each word, including the lemma, the part-of-speech tag, the shape of the word, and whether it is a stop word or number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This will take a while to run\n",
    "for dataset in [a, c, s, t]:\n",
    "    print(f\"Tokenizing text for dataset: {dataset['Campaign'][0]}\")\n",
    "    dataset['tokenized_text'] = dataset['Transcription'].apply(tokens)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create a new column containing the output of the `entities` function\n",
    "The `entities` function uses the previously loaded spaCy model to identify persons, places, organizations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE 1: This will take a while to run\n",
    "# NOTE 2: This cell is not required if only running the transcription visualization notebook\n",
    "for dataset in [a, c, s, t]:\n",
    "    print(f\"Identifying entities for dataset: {dataset['Campaign'][0]}\")\n",
    "    dataset['entities'] = dataset['Transcription'].apply(entities)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Optional: Preview the results of the `tokens` and `entities` functions for the first row of a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment a line and then run\n",
    "\n",
    "# ANTHONY\n",
    "#a.head(1)\n",
    "#a['tokenized_text'].iloc[0]\n",
    "#a['entities'].iloc[1000]\n",
    "\n",
    "# CATT\n",
    "#c.head(1)\n",
    "#c['tokenized_text'].iloc[0]\n",
    "#c['entities'].iloc[1000]\n",
    "\n",
    "# STANTON\n",
    "#s.head(1)\n",
    "#s['tokenized_text'].iloc[0]\n",
    "#s['entities'].iloc[1000]\n",
    "\n",
    "# TERRELL\n",
    "#t.head(1)\n",
    "#t['tokenized_text'].iloc[0]\n",
    "#t['entities'].iloc[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run the `separate_text` function to isolate tokens by category\n",
    "\n",
    "The `separate_text` function uses labels generated by the `spaCy` library to organize the contents of each transcription into actual text, stop words (conjunctions, prepositions, etc.), non-alphanumeric strings (punctuation, whitespace, etc.), numbers, and ambiguous words (when a transcriber cannot make out a word or character, a `?` will be used for the unknown character(s); this is reflected in the analyzed pattern of the word which is used to remove these words from the text category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the separate_text function on the Anthony data frame\n",
    "for dataset in [a, c, s, t]:\n",
    "    print(f\"Organizing tokens by category for: {dataset['Campaign'][0]}\")\n",
    "    separate_text(dataset)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Optional: Preview the results for the first six rows of the updated data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment a line and then run\n",
    "\n",
    "# ANTHONY\n",
    "#a.iloc[0:6]\n",
    "\n",
    "# CATT\n",
    "#c.iloc[0:6]\n",
    "\n",
    "# STANTON\n",
    "#s.iloc[0:6]\n",
    "\n",
    "# TERRELL\n",
    "#t.iloc[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process the Susan B. Anthony speech subset\n",
    "An inventory of speeches in the Susan B. Anthony Papers is available; this allows for subsetting the transcription data so that it can be processed and visualized separately from the entire transcription dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Extract the transcription text for the speeches\n",
    "The code below will group transcription data by the `ItemId`. The speech inventory will then be used to subset the transcription data using the `ItemId` of known speeches. The transcribed text will then be combined at the item level and stored in a dictionary that lists the `id`, `year`, `title`, and `text` of each speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the speech inventory\n",
    "a_speeches = load_csv(SPEECHES)\n",
    "\n",
    "# Group transcriptions by ItemId\n",
    "# Creates a dictionary where the ItemId is the key and the value is a list of associated row indexes\n",
    "a_groups = a.groupby('ItemId').groups\n",
    "\n",
    "# Create a list of dictionaries representing each speech\n",
    "# This structure is specifically designed for visualization in the next notebook\n",
    "speech_list = []\n",
    "\n",
    "for row in range(a_speeches.shape[0]):\n",
    "    d = re.findall('\\d{4}', a_speeches.iloc[row][1])\n",
    "    speech_id = a_speeches.iloc[row][0]\n",
    "    speech_text = []\n",
    "    for i in a_groups[speech_id]:\n",
    "        speech_text.extend(a['processed_text'].iloc[i])\n",
    "    speech = {'id': speech_id, \n",
    "              'year': d[0], \n",
    "              'title': a_speeches.iloc[row][2], \n",
    "              'text': speech_text}\n",
    "    speech_list.append(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Store the speech subset \n",
    "The following code will save the output `speech_list` into a variable that can be used across notebooks in this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store speech_list\n",
    "\n",
    "# Reuse the variable in another notebook using the following command\n",
    "# %store -r speech_list\n",
    "# Then call the variable like usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Optional: Save `speech_list` to a Python file for import or reuse beyond these notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional code to create a stand-alone file for the speech data\n",
    "with open('outputs/anthony_speech_lemmas.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"#! usr/bin/env python3\\n#-*- encoding: utf-8 -*-\\n\\n\")\n",
    "    f.write(\"speech_list = \" + str(speech_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process transcription data for all four datasets\n",
    "The following code will prepare the data similar to the Susan B. Anthony speech subset above. Running this code is necessary for visualizing at the dataset-level for all four datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create a list of all words from `processed_text` for each dataset\n",
    "This code will create a dictionary containing the titles and aggregated text from the `processed_text` column for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = []\n",
    "\n",
    "for dataset in [a, c, s, t]:\n",
    "    transcription_text = []\n",
    "    for row in range(dataset.shape[0]):\n",
    "        transcription_text.extend(dataset['processed_text'].iloc[row])\n",
    "    transcription = {'title': dataset['Campaign'][0],\n",
    "                     'text': transcription_text}\n",
    "    transcriptions.append(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Store the `transcriptions` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store transcriptions\n",
    "\n",
    "# Reuse the variable in another notebook using the following command\n",
    "# %store -r transcriptions\n",
    "# Then call the variable like usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Optional: Save `transcriptions` to a Python file for import or reuse beyond these notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional code to create a stand-alone file for the processed transcriptions data\n",
    "with open('outputs/transcriptions_lemmas.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"#! usr/bin/env python3\\n#-*- encoding: utf-8 -*-\\n\\n\")\n",
    "    f.write(\"transcriptions = \" + str(transcriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
