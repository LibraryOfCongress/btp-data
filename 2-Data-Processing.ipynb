{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import pandas as pd\n",
    "import re\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthony = \"data/anthony/susan-b-anthony-papers_2022-10-12.csv\"\n",
    "speech_inventory = \"data/anthony/anthony_speech_list.csv\"\n",
    "catt = \"data/catt/carrie-chapman-catt-papers_2022-10-12.csv\"\n",
    "stanton = \"data/stanton/elizabeth-cady-stanton-papers_2022-10-19.csv\"\n",
    "terrell = \"data/terrell/mary-church-terrell-advocate-for-african-americans-and-women_2023-01-20.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load each CSV file into a data frame.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(file, dtype=str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each csv and store the data frame in a variable\n",
    "a = load_csv(anthony)\n",
    "c = load_csv(catt)\n",
    "s = load_csv(stanton)\n",
    "t = load_csv(terrell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the load worked by previewing with `df.head()`\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "def tokens(text) -> list:\n",
    "    \"\"\"Runs NLP process on text input. \n",
    "    \n",
    "    Returns: \n",
    "        process (list): A list containing tuples of NLP attributes for each word in the transcription.\n",
    "    \"\"\"\n",
    "    doc = nlp(str(text))\n",
    "    process = ([(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                token.shape_, token.is_alpha, token.is_stop) for token in doc])\n",
    "\n",
    "    return process\n",
    "\n",
    "\n",
    "def entities(text) -> list:\n",
    "    \"\"\"Runs NER process on text input. \n",
    "    \n",
    "    Returns:\n",
    "        process (list): A list containing tuples of NER attributes for each word in the transciption.\n",
    "    \"\"\"\n",
    "    doc = nlp(str(text))\n",
    "    process = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column containing the output of the tokens function\n",
    "# NOTE: This will take a while to run\n",
    "a['tokenized_text'] = a['Transcription'].apply(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column containing the output of the entities function\n",
    "# NOTE: This will take a while to run\n",
    "a['entities'] = a['Transcription'].apply(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first row of the data\n",
    "a.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the tokenized text for the first row\n",
    "a['tokenized_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the tokenized text for the first row\n",
    "a['entities'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_text(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Adds new columns to the data frame then loops through the \n",
    "    tokenized text of each row moving each category to the newly created relevant column.\"\"\"\n",
    "    \n",
    "    # Add new columns to the data frame\n",
    "    for c in ['text', 'stop_words', 'nonalphanums', 'numbers', 'ambigs', 'processed_text']:\n",
    "        df[c] = pd.Series(dtype=str)\n",
    "    \n",
    "    # Make a copy of the tokenized text lists by row\n",
    "    for row in range(df.shape[0]):\n",
    "        text_block = df['tokenized_text'].iloc[row].copy()\n",
    "        \n",
    "        text = []\n",
    "        stop_words = []\n",
    "        nonalphanums = []\n",
    "        numbers = []\n",
    "        ambigs = []\n",
    "    \n",
    "        for idx, word in enumerate(text_block):\n",
    "            # Move stopwords\n",
    "            if word[7] == True:\n",
    "                stop_words.append(text_block[idx])\n",
    "            # Move punctuation and whitespace\n",
    "            elif word[2] in ['PUNCT', 'SPACE', 'CCONJ', 'X', 'SYM']:\n",
    "                nonalphanums.append(text_block[idx])\n",
    "            # Move numbers\n",
    "            elif word[2] == 'NUM':\n",
    "                numbers.append(text_block[idx])\n",
    "            # Move ambiguous transcribed words\n",
    "            elif '?' in word[5]:\n",
    "                ambigs.append(text_block[idx])\n",
    "            # Move text\n",
    "            else:\n",
    "                text.append(text_block[idx])\n",
    "                \n",
    "        df['text'].iloc[row] = text\n",
    "        df['stop_words'].iloc[row] = stop_words\n",
    "        df['nonalphanums'].iloc[row] = nonalphanums\n",
    "        df['numbers'].iloc[row] = numbers\n",
    "        df['ambigs'].iloc[row] = ambigs\n",
    "        # Add lowercase lemmas for all words in 'text'\n",
    "        df['processed_text'].iloc[row] = [i[1].lower() for i in a['text'].iloc[row]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the separate_text fucntion on the Anthony data frame\n",
    "separate_text(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first six rows of the updated data frame\n",
    "a.iloc[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start working with only the Susan B. Anthony speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the speech inventory\n",
    "a_speeches = load_csv(speech_inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group transcriptions by ItemId\n",
    "# Creates a dictionary where the ItemId is the key and the value is a list of associated row indexes\n",
    "a_groups = a.groupby('ItemId').groups\n",
    "\n",
    "# Create a list of dictionaries representing each speech\n",
    "# This structure is specifically designed for visualization in the next notebook\n",
    "speech_list = []\n",
    "for row in range(a_speeches.shape[0]):\n",
    "    d = re.findall('\\d{4}', a_speeches.iloc[row][1])\n",
    "    speech_id = a_speeches.iloc[row][0]\n",
    "    speech_text = []\n",
    "    for i in a_groups[speech_id]:\n",
    "        speech_text.extend(a['processed_text'].iloc[i])\n",
    "    speech = {'id': speech_id, \n",
    "              'year': d[0], \n",
    "              'title': a_speeches.iloc[row][2], \n",
    "              'text': speech_text}\n",
    "    speech_list.append(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the speech list as a reusable variable across notebooks\n",
    "%store speech_list\n",
    "\n",
    "# Reuse the variable in another notebook using the following command\n",
    "# %store -r speech_list\n",
    "# Then call the variable like usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
